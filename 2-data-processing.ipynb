{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"LAB2.ipynb","provenance":[{"file_id":"1Oi_asqq_7A12-Rbgi3Mf1NH4TMradDpC","timestamp":1603822785138}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"c4DmlrH60_Yq"},"source":["# Task 1: Data Prepping"]},{"cell_type":"markdown","metadata":{"id":"EBX0v02s1ip6"},"source":["## 1.1 - Loading the data \n","\n","- **_[TO DO]_**: Load the climate data from LAB1 (5 Danish cities between 1980-2018), and store them into Pandas data frames.\n","- **_[TO DO]_**: Have a quick overview of the downloaded dataset and make sure to understand the variables contained in the dataframe. Pay particular attention to the DateTime. "]},{"cell_type":"markdown","metadata":{"id":"r_LyErLr2ajE"},"source":["**[HINT]:** In this case, the index of the dataframe is not an integer (i.e., 0, 1, 2,...) as seen in the previous lab. But we have a multiple index (MultiIndex) [pandas.DataFrame.xs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.xs.html). Check the website to see how select the data of the desidered city.\n","    \n"]},{"cell_type":"code","metadata":{"id":"ATcPcJas2ajG"},"source":["###########################\n","# Tasks:\n","#   Load climate data from LAB1 \n","#   Quick overview of the dataset:\n","#       - understand the variables in the dataframe\n","#       - check the Data time for two cities \n","###########################\n","\n","\n","### TO DO\n","\n","\n","#########"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cx7BliAP2ajI"},"source":["## 1.2 - Data interpolation\n","\n","Many time series analyses assume observations over uniform time intervals. In the real world, incoming data often does not arrive in this way. As a result, one common prerequisite for Time Series analysis is to take an initially raw input and transform it into discrete intervals, or to resample an input at one frequency into an input of a different frequency.\n","\n","For instance, we plot the temperature data for the cities of Aalborg and Aarhus in the day 1st March 2018 from 00am to 12pm. We can notice that the temperature data for the two cities is not always recorded at the same time."]},{"cell_type":"code","metadata":{"id":"GsgrizXe2ajJ"},"source":["# List of the Danish cities in this data-set.\n","cities = ['Aalborg', 'Aarhus', 'Esbjerg', 'Odense', 'Roskilde']\n","\n","\n","city_1 = df.xs(cities[1])\n","city_2 = df.xs(cities[2])\n","\n","\n","plt.figure(figsize=(16, 7))\n","plt.plot(city_1.loc['2018-03-01 00:00:00':'2018-03-01 12:00:00', 'Temp'],\n","         label='Aarhus', linestyle='--', marker='o',  c='r')\n","\n","plt.plot(city_2.loc['2018-03-01 00:00:00':'2018-03-01 12:00:00', 'Temp'],\n","         label='Roskilde', linestyle='--', marker='o',  c='b')\n","\n","plt.xlabel('DateTime')\n","plt.ylabel('Temperature')\n","\n","plt.ylim([-8, -2])\n","plt.grid()\n","plt.legend()\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aknuqY392ajL"},"source":["As just observed, this dataset contains data recorded with an irregular interval across cities. Therefore, before continuing with the data analysis, we will [resample](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html) and [interpolate](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html) the values in order to have values at a **regular interval** for all the cities and make possible a fair comparison."]},{"cell_type":"code","metadata":{"id":"awbXVa782ajM"},"source":["def _resample(df):\n","\n","    # Upsample in order to have data for every minute.\n","    df_res = df.resample('1T')\n","\n","    # Interpolate through time\n","    df_res = df_res.interpolate(method='time')\n","\n","    # Downsample to a 1-hour basis (60 minutes).\n","    df_res = df_res.resample('60T')\n","\n","    df_res = df_res.interpolate()\n","\n","    # We can remove all empty rows or leave like this for missing values analysis?\n","#     df_res = df_res.dropna(how='all')\n","\n","    return df_res\n","\n","\n","def load_resampled_data():\n","\n","    # Path for the cache-file with the resampled data.\n","    path = path_resampled_data_pickle()\n","\n","    # Check if the cache-file already exists and load it eventually\n","    if os.path.exists(path):\n","        df = pd.read_pickle(path)\n","    else:\n","        # Otherwise resample the original data and save it in a cache-file.\n","\n","        # Load the original data.\n","        df_org = load_original_data()\n","\n","        # Split the original data into separate DataFrames for each city.\n","        df_cities = [df_org.xs(city) for city in cities]\n","\n","        # Resample the data for each city.\n","        df_resampled = [_resample(df_city) for df_city in df_cities]\n","\n","        # Join the resampled data into a single data-frame.\n","        df = pd.concat(df_resampled, keys=cities, axis=1, join='inner')\n","\n","        # Save the resampled data in a cache-file for quick reloading.\n","        df.to_pickle(path)\n","\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"Q53WJQAU2ajO"},"source":["def path_resampled_data_pickle():\n","    return os.path.join(data_dir, \"weather-denmark-resampled.pkl\")\n","\n","df_resampled = load_resampled_data()\n","df_resampled"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KJOL6sVJ2ajQ"},"source":["**_[TO DO]_:** Check now the previous plot after resampling and interpolation.\n"]},{"cell_type":"code","metadata":{"id":"cI7Iz5KO2ajQ"},"source":["###########################\n","# Task: \n","#   Plot the previous one with the new data\n","###########################\n","\n","\n","### TO DO\n","\n","\n","#########"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tzwmr4iVAGI1"},"source":["## 1.3 - Data overview\n","\n","We will now analyse the statistical properties of this dataset. Since this dataset is quite big and contains weather data of almost 40 years, in order try to complete the following instructions:\n","- **_[TO DO]_**  extract the general statistical properties (mean, variance, min value, max value),\n","- **_[TO DO]_**  for each city, plot the average temperature and the wind speed variance in January for all the years (1980 to 2018) \n","- **_[TO DO]_** the maximum temperature and the minimum wind speed in May "]},{"cell_type":"code","metadata":{"id":"SbDIkaEhAGI1"},"source":["###########################\n","# Task: \n","#   extract general statistical properties\n","#\n","###########################\n","\n","\n","### TO DO\n","\n","\n","#########"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B_RiAp5kAGI3"},"source":["###########################\n","# Task: \n","#   plot average temperature in January from 1980 to 2018.\n","#\n","###########################\n","\n","\n","### TO DO\n","\n","\n","#########"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qkZ7RsEWAGI5"},"source":["###########################\n","# Task: \n","#   plot average wind speed variance in January from 1980 to 2018.\n","#\n","###########################\n","\n","\n","### TO DO\n","\n","#########"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HOXHM8mqAGI6"},"source":["###########################\n","# Task: \n","#   plot maximum temperature in May from 1980 to 2018.\n","#\n","###########################\n","\n","\n","## TO DO\n","\n","\n","#########"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tdqOTTh6AGI8"},"source":["###########################\n","# Task: \n","#   plot minimum wind speed in May from 1980 to 2018.\n","#\n","###########################\n","\n","\n","### TO DO\n","\n","\n","#########"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GyfyRXeRAGI-"},"source":["**NOTE:** Look at the scale of the previous plots. Do you think all those zeros actually mean that there was no wind at all?"]},{"cell_type":"markdown","metadata":{"id":"NV9iGRvxAGI-"},"source":["## 1.4 - Outliers"]},{"cell_type":"markdown","metadata":{"id":"721STQcBAGI-"},"source":["In the plots you can notice there is definitely an outlier (you could also identify it using the general properties of the dataset as well). \n","\n","This is quite a common case one can encounter while dealing with big datasets. Such inconsistancyin the data is likely to affect our inference about the general statistics behind the data. **How to detect such outliers?**"]},{"cell_type":"markdown","metadata":{"id":"rIVA8GKBAGI_"},"source":["\n","### 1.4.1 Using visualisation tools\n","\n","The very first method is to spot outliers by using your eyes. For example, open your data and find a\n","datapoint is clearly different from others. Let's look at different visualisation toos.\n"]},{"cell_type":"markdown","metadata":{"id":"UCCJVAEQ-Hba"},"source":["#### 1.4.1.1 Box Plot\n","Boxplots are a standardized way of displaying the distribution of data based on a five number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”):\n","\n","![](Figures/box_plot.png)\n","\n","**median (Q2/50th Percentile)**: the middle value of the dataset.\n","\n","**first quartile (Q1/25th Percentile)**: the middle number between the smallest number (not the “minimum”) and the median of the dataset.\n","\n","**third quartile (Q3/75th Percentile)**: the middle value between the median and the highest value (not the “maximum”) of the dataset.\n","\n","**interquartile range (IQR)**: 25th to the 75th percentile.\n","\n","**“maximum”**: $Q3 + 1.5*IQR$\n","\n","**“minimum”**: $Q1 -1.5*IQR$\n","\n","**whiskers** (shown in blue)\n","\n","**outliers** (shown as green circles)"]},{"cell_type":"markdown","metadata":{"id":"Ci-tPp9MAGI_"},"source":["Back to our max temperature example, let's box-plot and find out the outlier in the data we used to plot \"The max temperature in May - city of Odense\"."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"3uMQGI9UAGI_"},"source":["import seaborn as sns\n","\n","# extract all the data except the last one because it is a NaN, which is missing value. \n","# We will see shortly how to deal with missing value.\n","\n","outlier_dataset = np.array(may_max_temp_3[:-1])\n","outlier_year = np.array(year[:-1])\n","\n","# box plot\n","sns.boxplot(outlier_dataset)\n","plt.xlabel(r'Temperature ($^\\circ$C)')\n","plt.xlim([10, 60])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5v6s3YXRAGJB"},"source":["As you can see, besides the outlier temperature 50 degrees, there is another outlier temperature around 31 degrees. Is it really an outlier? For 31 degres, it is ambiguous whether it is an outlier or variance."]},{"cell_type":"markdown","metadata":{"id":"ocZNzypxAGJB"},"source":["#### 1.4.1.2 Scatter Plot\n","A scatter plot is the collection of points that shows values for two variables. \n","\n"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"KZIrzF8aAGJB"},"source":["# scatter plot: x=year,y=value\n","plt.scatter(outlier_year,outlier_dataset)\n","plt.xlabel('Year')\n","plt.ylabel('Maximum temperature in May - city of Odense')\n","plt.ylim([15, 55])\n","plt.grid()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LMPiGOOBAGJD"},"source":["Observe how the data point around 50 degrees seems to clearly deviate from the general case."]},{"cell_type":"markdown","metadata":{"id":"_oV9JrD4AGJD"},"source":["### 1.4.2 Using statistics\n","\n","Statistical methods can also be very useful to detec outliers. Let's look at some examples."]},{"cell_type":"markdown","metadata":{"id":"ubsld1_k-QhR"},"source":["#### 1.4.2.1 Z-score\n","The Z-score is the signed number of standard deviations, calculated by following equation:\n","\n","$z=\\frac{x-\\overline{x}}{S}$\n","\n","where $\\overline{x}$ is the mean value, $S$ is the standard deviation.\n","\n","Let's see how it works in python."]},{"cell_type":"code","metadata":{"id":"sjJ5ceYYAGJD"},"source":["from scipy import stats\n","# calc the abs value of z-score\n","z = np.abs(stats.zscore(outlier_dataset))\n","\n","print('z score of the dataset is:\\r\\n',z)\n","\n","plt.plot(outlier_year,z)\n","plt.grid()\n","plt.ylim([0, 6])\n","plt.ylabel('Z score')\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hsw0vjn7AGJF"},"source":["But, then what? how can we find outlier using these z-scores?\n","\n","In general, we **put a threshold on z-scores**. Typically, the threshold is 3.\n","If $|z\\_score|>3$, then it is an outlier."]},{"cell_type":"code","metadata":{"id":"qnYBrCSoAGJF"},"source":["# set a threshold and find the location where the value meets our condition(s)\n","threshold = 3\n","outlier_loc = np.where(z > threshold)\n","\n","# find the outlier value given its index\n","outlier_by_Z_Score = outlier_dataset[outlier_loc]\n","print('the data classified as outlier by z score:\\r\\n', outlier_by_Z_Score)\n","print('the year of the outlier is:\\r\\n', outlier_year[outlier_loc])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W6fm_CVEAGJH"},"source":["#### 1.4.2.2   IQR score\n","Box plot use the IQR method to display data and outliers(shape of the data) but in order to be get a list of identified outlier, we will need to use the mathematical formula and retrieve the outlier data."]},{"cell_type":"code","metadata":{"id":"XCed2HZyAGJH"},"source":["Q1 = np.quantile(outlier_dataset,0.25)\n","Q3 = np.quantile(outlier_dataset,0.75)\n","IQR = Q3-Q1\n","Minimum = Q1-1.5*IQR\n","Maximum = Q3+1.5*IQR\n","\n","# find values that meets the conditions: (outlier_dataset<Minimum) or (outlier_dataset>Maximum)\n","outlier_by_IQR_Score=outlier_dataset[(outlier_dataset<Minimum) | (outlier_dataset>Maximum)]\n","print('The data classified as outlier by IQR score:\\r\\n', outlier_by_IQR_Score)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YPhMrcyRAGJI"},"source":["## 1.4.3 What to do with outliers?\n","\n","Now that we have ways to detecting outliers, what should we do with them? Let's go through a few potential solutions. "]},{"cell_type":"markdown","metadata":{"id":"RW76w3G4_EJH"},"source":["### 1.4.3.1 Drop them\n","Simply drop the outliers as long as we have sufficient amount of data left.\n","\n","Assume we have detected the outlier using z score. We can then drop it easily."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"PQqWH8W_AGJI"},"source":["# find values that meet the conditions: outlier_dataset != outlier_by_Z_Score\n","# **NOTE** conditions like \"array!=value\" and  \"array==value\" only works \n","# when value is a single value, not an array. The same mechanism for np.where(array==value).\n","\n","dropped_outlier_dataset = outlier_dataset[outlier_dataset != outlier_by_Z_Score]\n","\n","# do the same for year value\n","dropped_outlier_year = outlier_year[outlier_dataset != outlier_by_Z_Score]\n","\n","print('Before Drop, the dataset has shape:\\r\\n', outlier_dataset.shape)\n","print('After Drop, the dataset has shape:\\r\\n', dropped_outlier_dataset.shape)\n","\n","# plot and compare them\n","fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 5))\n","ax1.set_title('Before Drop')\n","ax1.scatter(outlier_year,outlier_dataset)\n","ax1.scatter(outlier_year[outlier_loc], outlier_dataset[outlier_loc], c='r')\n","ax1.set_ylim([15, 55])\n","ax1.set_xlabel('Years')\n","ax1.set_ylabel(r'Temperature ($^\\circ$C)')\n","ax1.grid()\n","\n","ax2.set_title('After Drop')\n","ax2.scatter(dropped_outlier_year,dropped_outlier_dataset)\n","ax2.set_xlabel('Years')\n","ax2.grid()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L5jm-2bZAGJK"},"source":["### 1.4.3.2 Cap your outliers.\n","Another way to handle true outliers is to cap them. \n","\n","Sometimes, the outlier appears as it hits the upper/lower bound of your sensor. What we can do is to replace it with a setting value, which, for example, could be the max/min value excluding those outliers or just some pre-fixed value.\n","\n","Let's have a look how to do that in python."]},{"cell_type":"code","metadata":{"id":"pc_TT2WhAGJK"},"source":["capped_outlier_dataset=np.copy(outlier_dataset)\n","print('Before cap the outlier, its value:\\r\\n',capped_outlier_dataset[outlier_loc])\n","# cap the outliers\n","capped_outlier_dataset[outlier_loc]=np.max(dropped_outlier_dataset)\n","print('After cap the outlier, its value:\\r\\n',capped_outlier_dataset[outlier_loc])\n","\n","# plot and compare them\n","fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 5))\n","ax1.set_title('Before Cap')\n","ax1.scatter(outlier_year,outlier_dataset)\n","ax1.scatter(outlier_year[outlier_loc], outlier_dataset[outlier_loc], c='r')\n","ax1.set_ylim([15, 55])\n","ax1.set_xlabel('Years')\n","ax1.set_ylabel(r'Temperature ($^\\circ$C)')\n","ax1.grid()\n","\n","ax2.set_title('After cap')\n","ax2.scatter(outlier_year,capped_outlier_dataset)\n","ax2.scatter(outlier_year[outlier_loc], capped_outlier_dataset[outlier_loc], c='r')\n","ax2.set_xlabel('Years')\n","ax2.grid()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oZfxrq8yAGJL"},"source":["### 1.4.3.3 Replace it with a new value.\n","If an outlier seems to be due to a mistake in your data, you can try imputing another value. Common imputation methods include using the mean of a variable or utilizing a regression model to predict the missing value.\n","\n","For example, we can replace it with the prediction value by a linear regression model.\n","\n","Let's have a look how to do that in python."]},{"cell_type":"code","metadata":{"id":"HaDP4jGrAGJM"},"source":["replaced_outlier_dataset = np.copy(outlier_dataset)\n","print('Before replacing, the outlier value is:\\r\\n',replaced_outlier_dataset[outlier_loc])\n","\n","# find the coefficient of our linear model using clean data\n","z = np.polyfit(dropped_outlier_year,dropped_outlier_dataset, deg=1) # deg=1: use 1-order polynomial regression model\n","# define our linear model\n","pred_func = np.poly1d(z)\n","# predict the value of our outlier and replace it\n","pred_value = pred_func(outlier_year[outlier_loc])\n","replaced_outlier_dataset[outlier_loc]=pred_value\n","print('After replacing, the outlier value predicted by a regression model is: \\r\\n',replaced_outlier_dataset[outlier_loc])\n","\n","# plot and compare them\n","fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 5))\n","ax1.set_title('Before replace')\n","ax1.scatter(outlier_year,outlier_dataset)\n","ax1.scatter(outlier_year[outlier_loc], outlier_dataset[outlier_loc], c='r')\n","ax1.set_ylim([15, 55])\n","ax1.set_xlabel('Years')\n","ax1.set_ylabel(r'Temperature ($^\\circ$C)')\n","ax1.grid()\n","\n","ax2.set_title('After replace')\n","ax2.scatter(outlier_year,replaced_outlier_dataset)\n","ax2.scatter(outlier_year[outlier_loc], pred_value, c='r')\n","ax2.set_xlabel('Years')\n","ax2.grid()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jm2ANRa9AGJN"},"source":["**_[TO DO]_** Congratulations!!! You have learned the techniques needed to detect and deal with outliers. Now, try to detect and deal with outliers in the same dataset showed above, but:\n","\n","- use the IQR score to detect the outliers\n","- use the temperature data for the months in which the outliers are found to predict and replace them.\n","- use a higher order polynomial (2nd or 3rd) to fit the data.\n","\n","**HINT:** \n","\n","1. Find the years that contain outliers, the IQR score might give you different results compared to the z-score\n","2. Are all the data points detected by the IQR score TRUE outliers?\n","3. Check the data of the months in which the outliers are found to see if there are more (i.e. the maximum value being an outlier does not necessarily mean it is the only one in that month!)\n","4. Use the \"good\" data to fit a 2nd or 3rd order polynomial and estimate the outliers."]},{"cell_type":"code","metadata":{"id":"ZMTlrqS8AGJN"},"source":["##########\n","# - use the IQR score to detect the outliers\n","# - use the temperature data for the months in which the outliers are found to predict and replace them.\n","# - use a higher order polynomial (2nd or 3rd) to fit the data.\n","\n","# TO BE COMPLETED \n","\n","##########################################################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PtKalbkDAG-n"},"source":["## 1.5 Missing data"]},{"cell_type":"markdown","metadata":{"id":"lH8R_IAuAZuY"},"source":["Resampling a dataset does not assure that no missing values will be found afterwards, because the interpolation method incurs in problems when many consecutive NaNs are encountered. In this case, you can see that there are some NaNs in the cities of Esbjerg and Roskilde, but what about the other cities? (remember we were only looking at the head of the dataset, but it actually has more than 330000 rows!)\n","\n","**_[TO DO]_** count the number of missing values in each column of the dataset."]},{"cell_type":"code","metadata":{"id":"RVAK8YwRAGJO"},"source":["# task\n","#     count the number of missing values in each column of the dataset.\n","\n","# TO DO\n","##############################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9KyO1qfLAGJQ"},"source":["You can clearly see that the number of missing values is less than 0.5% and only for the pressure in Esbjerg and Roskilde. This does not affect the dataset as a whole, but since it is a time series, it can affect statistics on a yearly (or seasonal) time basis. \n","\n","Moreover, a second consideration about the linear interpolation in the resampling step is that the filling process might be deceiving.\n","\n","**_[TO DO]_** As an example, plot only the signals that in the previous question contained a significant amount of missing values. Can you spot where the linear interpolation does not provide a reliable outcome?"]},{"cell_type":"code","metadata":{"id":"fQxuqt7sAGJQ"},"source":["# TO DO\n","\n","########################################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K0NHCsoxAGJR"},"source":["**_[TO DO]_** For just one of the two signals, produce a plot of the atmospheric pressure versus time and an histogram distribution for two different years:\n","- one with mostly \"reliable\" measurements\n","- one with an excessive amount of linearly interpolated values\n","\n","Which differences can you see in those plots?\n","\n","**HINT:** The typical value of the atmospheric pressure is around 1013 mbar and it has small variance. Therefore the distribution should be a Gaussian."]},{"cell_type":"code","metadata":{"id":"k1QvlaNgAGJR"},"source":["# [TO DO (1)] choose one year with many interpolated values, plot it and make an histogram of the distribution\n","# TO BE COMPLETED\n","\n","###############################################"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jvpXbW9cAGJS"},"source":["# [TO DO (2)] choose one year with mostly actual observations, plot it and make an histogram of the distribution\n","\n","# TO BE COMPLETED"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kdwViR3zAGJT"},"source":["### 1.5.1 Imputation of missing data points\n","\n","Dealing with missing data points (or badly interpolated ones) is not easy. When missing data points are found, there are various approaches that we can consider:\n","\n","- drop all of them (in this case is quite reasonable as they are only a few, percentage-wise).\n","- replace with the mean, median or mode. \n","- replace with a fixed number, like -1 or 0.\n","\n","Since we are dealing with a time series, none of these methods seems to be appropriate, especially the replacement. \n","Therefore, we could generate this data by creating a predictive model that estimates the missing data from all the other input signals. Then we could fill the gaps by putting these generated values back into the data-set.\n","\n","This process is also referred to as **imputation**. There are many ways to impute the missing values, one of the most powerful being the *MissForest* module inside the [missingpy](https://pypi.org/project/missingpy/) package (this package is not installed in the base Anaconda environment).\n","\n","It basically fits a Random Forest (shallow machine learning algorithm) to the data and estimates all the missing values in each column feature. Here we only provide the code to run the MissForest algorithm (which is quite slow on Jupyter!).\n","\n","\n","As an example, we have pre-trained the MissForest in order to predict the pressure values in Roskilde in 1982 (all the data points are linearly interpolated).\n","\n","You can see that the imputation consistently replicates how the pressure varies through the year, with a higher variance in winter and a lower one in summer. \n","\n","Feel free to install the *missingpy* package (you need to use pip, is not in the Anaconda repository) and try to replace the missing values and the badly interpolated ones."]},{"cell_type":"code","metadata":{"id":"17mfU7xOAGJU"},"source":["df_Esbjerg_imputed = pd.read_pickle('./weather-data/weather_Esbjerg_imputed.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"paFLnQkjAGJV"},"source":["plt.figure(figsize=(16, 7))\n","df_resampled['Esbjerg'].loc['1980-03-01 10:00:00':'1995-12-31 23:00:00','Pressure'].plot()\n","df_Esbjerg_imputed.loc['1982-01-01 00:00:00':'1982-12-31 23:00:00','Pressure'].plot(alpha=0.7)\n","\n","plt.ylim([960, 1060])\n","plt.ylabel('Pressure (mbar)')\n","plt.show()"],"execution_count":null,"outputs":[]}]}